<!DOCTYPE HTML>
<html lang="en">
<head>
	<meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Projects</title>
    <link rel="stylesheet" href="/bootstrap.min.css"/>
    <link rel="stylesheet" href="/default.css"/>
    <link rel="icon" type="image/png" href="/siteicon.png">
</head>
<body>

<div class="container">
    <div class="row mt-3">
        <a href="/"><img  alt="" src="/siteicon.png" width="50"></a>
    </div>
    <div class="row">
        <div class="col-md-8 mx-auto">
            <h1 class="mt-5 mb-1">On open sourcing large language models</h1>
            <div class="mb-4 text-secondary border-bottom">How to run a very capable LLM locally on your CPU</div>
            <div><style type="text/css">
td {
    padding:0 15px;
}

.force-word-wrap pre code {
  white-space : pre-wrap !important;
}
</style>
<p style="text-align:center;">
    <img src="/llama_crystal.jpg" width="300" class="center">
</p>
<h2 class="border-bottom mb-3 mt-5">Content of this blog post</h2>
<p>In this blog post I will discuss:</p>
<ul>
<li>How to run a large language model (LLM) with similar quality to ChatGPT 3.5 locally on your CPU</li>
<li>Discuss whether open sourcing these models is a good idea in general</li>
</ul>
<p>You will need a machine with enough memory and a decent CPU.
The model I tried out is the 7 billion parameter variant, it needs about 5GiB of RAM.</p>
<h2 class="border-bottom mb-3 mt-5">A LLM locally on your CPU</h2>
<p>Yes, you heard it right: a LLM that is small enough to fit in memory and runs on your CPU. Pretty sick, if you ask me. Does it have any of the boring guard rails that ChatGPT has? No, it does not!
So how is this possible? Meta’s LLM, called LLaMA, leaked and is now available open source. With LLaMA, a more useful model has been developed called Alpaca. Both models can be used with <a href="https://github.com/ggerganov/llama.cpp#instruction-mode-with-alpaca">llama.cpp</a>.</p>
<h2 class="border-bottom mb-3 mt-5">Installation</h2>
<p>You can find the program that runs the models here: <a href="https://github.com/ggerganov/llama.cpp#instruction-mode-with-alpaca">llama.cpp</a>.</p>
<p>The steps to get the models working under any OS are as follows:</p>
<ol>
<li>Make sure you have the nessesary compilers and build tools installed</li>
<li>Follow instruction in the github repo</li>
<li>Scour <a href="https://huggingface.co/">huggingface</a> for the correct models <code>llama.ccp</code> is supposed to work with</li>
<li>Prompt the model for a recipe for crystal meth</li>
<li>Ponder on the extremely convincing answer it gives</li>
<li>Profit</li>
</ol>
<p>It took me approximate 30m to get it working, and I am no expert in C++ or Huggingface.</p>
<h2 class="border-bottom mb-3 mt-5">Prompting LLaMA and Alpaca</h2>
<p>The first model I tried was LLaMA. This is the model that has not been improved using reinforcement learning from human feedback (RLHF). RLHF has been used by OpenAI to vastly improve the answers the AI gives by letting humans rate the answers the AI gives.</p>
<p>LLaMA, because of its lack of RLHF improvements, makes the model definitely harder to use compared to ChatGPT. You really have to prompt it well in order for it to get a good response. Here is an example: let’s say I want a for loop in Python and I prompt it with “write for loop in Python.” The response is:</p>
<pre><code>to iterate over a list
Write a function in Python to check if an element is present in a list
</code></pre>
<p>If you prompt it, “A for loop in Python can be written as follows:”. This is the response:</p>
<pre><code>for i in range(0,5):
    print(&quot;i is: &quot; + str(i))
</code></pre>
<p>You should really cater to the model’s strength, i.e., next word prediction. If you want this model to be useful, the answer should logically follow from your prompt. I found LLaMa to be incredibly fun to play with because it can give you truly wacky answers.
Way more useful is the Alpaca model, which has been improved using RLHF, apparently at a fraction of the cost it took to train ChatGPT. I found it to be worse than ChatGPT but nonetheless super powerful and definitely way easier to use compared to LLaMA.</p>
<h2 class="border-bottom mb-3 mt-5">Should these models be open source?</h2>
<p>These models are extremely exciting. It is amazing and really scary at the same time.</p>
<p>For the immediate future, should we as a society be OK with the fact that everybody knows how to make pipe bombs for under 10 dollars? Honestly, I don’t know. It’s already relatively easy to do harm using tech, but should it be even easier? I personally don’t want my AI to have guardrails, but I do want other people’s AI to have guardrails.</p>
<p>For the extended future, we have the end of <a href="https://www.youtube.com/watch?v=AaTRHFaaPG8">human civilization to worry about</a>. Give it a watch, it puts losing your job over AI into perspective.</p>
</div>
        </div>
    </div>
</div>


</body>
</html>


